# file pipe sender over udp 


file â†’ decode â†’ preprocess â†’ Hailo NN â†’ postprocess â†’ overlay â†’ H264 encode â†’ RTP â†’ UDP.




## receiver:

```
gst-launch-1.0  udpsrc port=5000 caps="application/x-rtp, media=video, encoding-name=H264, clock-rate=90000, payload=96" !   rtph264depay ! h264parse ! avdec_h264 ! videoconvert ! autovideosink sync=false

```

## sender:

```
python3 hailo_udp_pipeline.py \
  --input /data/my_video.mp4 \
  --hef /models/my_yolo.hef \
  --post /libs/libmy_post.so \
  --config /configs/my_yolo.json \
  --host 192.168.1.50 \
  --port 6000 \
  --bitrate 6000

```

Here is the **full explanation of the pipeline you gave**, step-by-step, from **file â†’ decode â†’ preprocess â†’ Hailo NN â†’ postprocess â†’ overlay â†’ H264 encode â†’ RTP â†’ UDP**.


---

# ğŸŸ¦ **1. Input video from file**

```bash
filesrc location=/local/.../detection.mp4 name=src_0 !
decodebin !
```

* `filesrc` reads an MP4 file from disk.
* `decodebin` automatically detects and loads the correct decoder:

  * H.264 â†’ h264parse â†’ v4l2h264dec (Rockchip)
  * H.265 â†’ appropriate decoders
  * etc.

â¡ï¸ Output is a **raw video stream**, e.g. `video/x-raw` in some YUV or RGB color format.

---

# ğŸŸ¦ **2. Preprocessing (scale + convert)**

### Queue #1

```bash
queue leaky=no max-size-buffers=30 ...
```

* Decouples decoding thread from preprocessing thread.
* Large buffer size because reading from file is bursty.

---

### Scale the frame

```bash
videoscale qos=false n-threads=2 !
video/x-raw, pixel-aspect-ratio=1/1 !
```

* `videoscale`: resize frames (even if not forced â€” ensures consistent geometry).
* Par=1/1 â†’ square pixels.

â¡ï¸ Ensures frame is normalized before Hailo inference.

---

### Queue #2

```bash
queue leaky=no max-size-buffers=30 ...
```

Keeps everything stable even if CPU spikes.

---

### Convert colorspace

```bash
videoconvert n-threads=2 qos=false !
```

Hailo inference almost always expects **RGB**.

* If decoder outputs NV12/YUYV/I420, this converts it.

â¡ï¸ Converts raw video into the **format the network expects**.

---

### Queue #3

```bash
queue leaky=no max-size-buffers=30 ...
```

Buffers between CPU and NPU.

---

# ğŸŸ¦ **3. Hailo Neural Network inference**

```bash
hailonet hef-path=.../yolov5m_wo_spp_60p.hef \
         batch-size=1
         nms-score-threshold=0.3
         nms-iou-threshold=0.45
         output-format-type=HAILO_FORMAT_TYPE_FLOAT32 !
```

This is the **Hailo NPU engine**.

It takes the raw video frame â†’ runs inference using the HEF.

* `batch-size=1` â†’ process one frame at a time (default for video).
* Score & IoU thresholds â†’ NMS is done internally.
* Output format FP32 â†’ ensures post-process `.so` supports it.

ğŸ‘‰ Output is **video frame + raw neural-network metadata attached to buffer**.

---

### Queue #4

Isolates the inference from post-processing.

---

# ğŸŸ¦ **4. Post-processing (C++ shared object)**

```bash
hailofilter function-name=yolov5 \
            so-path=.../libyolo_hailortpp_post.so \
            config-path=.../yolov5.json \
            qos=false !
```

This loads your **postprocess plugin written in C/C++**.

The `.so` takes the NN raw outputs (tensors) and turns them into:

* Bounding boxes
* Class IDs
* Confidences

and attaches them as **structured Hailo metadata**.

`config-path=yolov5.json` describes tensor shapes / anchors / strides etc.

â¡ï¸ You now have **meaningful detection objects per frame**.

---

### Queue #5

Ensures stable flow before overlay.

---

# ğŸŸ¦ **5. Overlay (draw bounding boxes)**

```bash
hailooverlay qos=false !
```

Uses the metadata added by `hailofilter` and draws:

* bounding boxes
* labels
* confidence values
* shapes (if pose model)

Overlay works only because metadata is inside buffer.

â¡ï¸ You now have **annotated frames** ready to encode.

---

### Queue #6

```bash
queue leaky=no max-size-buffers=30 ...
```

Last decoupling before encoding.

---

# ğŸŸ¦ **6. Convert to encoder-friendly format**

```bash
videoconvert n-threads=2 qos=false !
```

Encoders typically want I420 or NV12.
Overlay usually outputs ARGB or RGB.

â¡ï¸ This makes sure the frame is encoder-compatible.

---

# ğŸŸ¦ **7. Encode to H.264**

```bash
x264enc tune=zerolatency speed-preset=ultrafast bitrate=4000 key-int-max=30 !
```

This is a **real-time low-latency encoder**:

* `tune=zerolatency` â†’ no B-frames, no latency pipeline
* `speed-preset=ultrafast` â†’ fastest encoding at cost of compression quality
* `bitrate=4000` â†’ ~4 Mbps
* `key-int-max=30` â†’ one keyframe every 30 frames (for robustness in streaming)

â¡ï¸ Produces **H.264-encoded video**.

---

# ğŸŸ¦ **8. RTP packetization**

```bash
rtph264pay config-interval=1 pt=96 !
```

Wraps H.264 stream into RTP packets.

* `config-interval=1` â†’ periodically send SPS/PPS in-band
  (very important for UDP clients that join mid-stream)

â¡ï¸ You get **RTP/H264 packets** over UDP.

---

# ğŸŸ¦ **9. Send to network (UDP sink)**

```bash
udpsink host=<IP> port=5000
```

This transmits the RTP packets to a remote machine.

* <IP> = destination IP
* port = 5000

Your Python version exposes these as parameters.

---

# ğŸ“Œ Full Functional Flow

```
FILE â†’ DECODER â†’ SCALE â†’ CONVERT â†’ HAILO INFERENCE
     (yolo HEF) â†’ POSTPROCESS (C++ .so)
     â†’ OVERLAY â†’ FORMAT CONVERT â†’ H.264 ENCODE
     â†’ RTP PACKETIZE â†’ UDP STREAM OUT
```

---

# ğŸ“Œ Why this pipe is structured like this

### âœ” Maximum performance

Queues isolate CPU from NPU from encoder.

### âœ” Rockchip-friendly

Decoding â†’ scaling â†’ converting all done via CPU-optimized GStreamer plugins.

### âœ” Hailo workflow

hailonet â†’ hailofilter â†’ hailooverlay is the standard triplet.

### âœ” Real-time streaming

x264enc (zerolatency) + RTP + UDP ensures almost-zero overhead.

---

